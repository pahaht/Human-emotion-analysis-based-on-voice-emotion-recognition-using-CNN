# Human-emotion-analysis-based-on-voice-emotion-recognition-using-CNN

![alt text]( https://github.com/pahaht/Human-emotion-analysis-based-on-voice-emotion-recognition-using-CNN/blob/main/pictures/SystemScheme.JPG)

### *Introduction*
Speech Emotion Recognition (SER) is a technology that uses various algorithms 
to detect emotions from audio signals. This project uses FFT to convert time-domain
speech signals into the frequency domain, which is then processed by a CNN to classify the emotions.

### *Features*
FFT Processing: Converts speech signals from time-domain to frequency-domain.
CNN Model: Utilizes a deep learning model to classify emotions.
Emotion Categories: Supports multiple emotion categories (e.g., happy, sad, angry, neutral).
User-Friendly Interface: Easy-to-use interface for testing and evaluating speech samples.


Model Architecture
The model consists of the following main components:
FFT Layer: Converts the audio signal from time-domain to frequency-domain.
CNN Layers: Extracts features from the frequency-domain representation of the audio signal.
Fully Connected Layers: Classifies the extracted features into predefined emotion categories.

### *Dataset*
